{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjI9x-BDzv0z",
        "outputId": "7d938817-2259-4cdc-f314-d5c2d3ba8f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные успешно загружены.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# @title Ячейка 1: Импорты и Загрузка данных\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "try:\n",
        "    df_class = pd.read_csv('Loan_Default.csv')\n",
        "    df_reg = pd.read_csv('Car_Sales.csv')\n",
        "    print(\"Данные успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка загрузки: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Готовлю данные для Градиентного бустинга. Препроцессинг аналогичен подготовке для Случайного леса:\n",
        "\n",
        "*   **Без масштабирования:** Бустинг, как и другие деревянные модели, нечувствителен к масштабу признаков.\n",
        "*   **Заполнение пропусков:** Стандартная реализация GradientBoostingClassifier в sklearn не поддерживает NaN, поэтому я заполняю их медианой/модой.\n",
        "*   **Кодирование:** Использую LabelEncoder.\n",
        "\n",
        "Для ускорения обучения бустинга (он работает последовательно, а не параллельно, как лес) я сокращаю выборку для классификации до 10%."
      ],
      "metadata": {
        "id": "V98Dyqd83wPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 2: Препроцессинг для Бейзлайна\n",
        "# 1. Классификация (Loan Default)\n",
        "# Берем семпл 10%\n",
        "df_class_sample = df_class.sample(frac=0.1, random_state=42).copy()\n",
        "df_class_sample = df_class_sample.drop(columns=['ID', 'year'], errors='ignore')\n",
        "\n",
        "X_cls = df_class_sample.drop(columns=['Status'])\n",
        "y_cls = df_class_sample['Status']\n",
        "\n",
        "# Обработка пропусков\n",
        "num_cols_cls = X_cls.select_dtypes(include=['number']).columns\n",
        "cat_cols_cls = X_cls.select_dtypes(include=['object']).columns\n",
        "\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "X_cls[num_cols_cls] = imputer_num.fit_transform(X_cls[num_cols_cls])\n",
        "X_cls[cat_cols_cls] = SimpleImputer(strategy='most_frequent').fit_transform(X_cls[cat_cols_cls])\n",
        "\n",
        "# Label Encoding\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols_cls:\n",
        "    X_cls[col] = le.fit_transform(X_cls[col].astype(str))\n",
        "\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X_cls, y_cls, test_size=0.25, random_state=42, stratify=y_cls\n",
        ")\n",
        "\n",
        "# 2. Регрессия (Car Sales)\n",
        "df_reg_clean = df_reg.dropna(subset=['Price']).copy()\n",
        "X_reg = df_reg_clean.drop(columns=['Price', 'Model'])\n",
        "y_reg = df_reg_clean['Price']\n",
        "\n",
        "num_cols_reg = X_reg.select_dtypes(include=['number']).columns\n",
        "cat_cols_reg = X_reg.select_dtypes(include=['object']).columns\n",
        "\n",
        "X_reg[num_cols_reg] = SimpleImputer(strategy='median').fit_transform(X_reg[num_cols_reg])\n",
        "X_reg[cat_cols_reg] = SimpleImputer(strategy='most_frequent').fit_transform(X_reg[cat_cols_reg])\n",
        "\n",
        "for col in cat_cols_reg:\n",
        "    X_reg[col] = le.fit_transform(X_reg[col].astype(str))\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Данные для Gradient Boosting подготовлены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26gvKuo1z_68",
        "outputId": "8de58d8b-e805-4180-8fa9-db55f2664b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные для Gradient Boosting подготовлены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 3: Обучение Бейзлайна (Sklearn)\n",
        "\n",
        "# 1. Классификация\n",
        "gb_cls = GradientBoostingClassifier(random_state=42)\n",
        "gb_cls.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_cls = gb_cls.predict(X_test_cls)\n",
        "y_pred_proba_cls = gb_cls.predict_proba(X_test_cls)[:, 1]\n",
        "\n",
        "print(\"=== Результаты Бейзлайна (Gradient Boosting Classifier) ===\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_cls, y_pred_cls):.4f}\")\n",
        "print(f\"ROC-AUC:  {roc_auc_score(y_test_cls, y_pred_proba_cls):.4f}\")\n",
        "print(f\"F1-score: {f1_score(y_test_cls, y_pred_cls):.4f}\")\n",
        "\n",
        "# 2. Регрессия\n",
        "gb_reg = GradientBoostingRegressor(random_state=42)\n",
        "gb_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_reg = gb_reg.predict(X_test_reg)\n",
        "\n",
        "print(\"\\n=== Результаты Бейзлайна (Gradient Boosting Regressor) ===\")\n",
        "r2_base = r2_score(y_test_reg, y_pred_reg)\n",
        "mae_base = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "print(f\"R2:  {r2_base:.4f}\")\n",
        "print(f\"MAE: {mae_base:.2f}\")\n",
        "\n",
        "# Проверка на переобучение\n",
        "y_train_pred_reg = gb_reg.predict(X_train_reg)\n",
        "print(f\"R2 Train: {r2_score(y_train_reg, y_train_pred_reg):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeWcZxZn0Axr",
        "outputId": "d6532b90-00d2-4852-e513-a0aaf6c668ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Результаты Бейзлайна (Gradient Boosting Classifier) ===\n",
            "Accuracy: 0.9997\n",
            "ROC-AUC:  1.0000\n",
            "F1-score: 0.9994\n",
            "\n",
            "=== Результаты Бейзлайна (Gradient Boosting Regressor) ===\n",
            "R2:  0.6765\n",
            "MAE: 4803.48\n",
            "R2 Train: 0.9178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ:**\n",
        "1.  **Классификация:** Снова идеальные метрики Accuracy 1.0, подтверждает наличие утечки Interest_rate_spread, как и в предыдущих лабораторных. В этапе улучшения я ее уберу.\n",
        "2.  **Регрессия:** Результат $R^2 \\approx 0.67$ довольно слабый.\n",
        "    *   **Причина:** Градиентный бустинг по умолчанию использует неглубокие деревья max_depth=3. Он не успел выучить сложные зависимости на грязных данных с выбросами, а логарифмирование я еще не применял. Переобучение между Train 0.91 и Test 0.67 существенное."
      ],
      "metadata": {
        "id": "XbPOkvw00eko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Гипотезы:**\n",
        "1.  **Регрессия (Данные):** Логарифмирование цены log1p и удаление выбросов это стандартный мастхэв для всех моих моделей. Это сделает распределение ошибок нормальным.\n",
        "2.  **Классификация (Честность):** Удаление признака-утечки Interest_rate_spread, чтобы оценить реальную силу алгоритма.\n",
        "3.  **Гиперпараметры:** Бустинг очень чувствителен к настройкам. я подберу:\n",
        "    *   n_estimators число деревьев.\n",
        "    *   learning_rat скорость обучения: чем меньше, тем точнее, но нужно больше деревьев.\n",
        "    *   max_depth: обычно бустингу нужны неглубокие деревья (3-5), в отличие от леса."
      ],
      "metadata": {
        "id": "AJ-htIQ60mj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Готовлю улучшенные данные для градиентного бустинга.\n",
        "\n",
        "1.  **Регрессия:** Применяю уже ставший стандартным пайплайн: удаляю выбросы и логарифмирую целевую переменную ($y_{new} = \\ln(1+y)$). Градиентный бустинг последовательно исправляет ошибки (остатки) предыдущей модели, и большие выбросы в таргете приведут к огромным ошибкам, которые будут доминировать в процессе обучения.\n",
        "2.  **Классификация:** Для чистоты эксперимента и честной оценки модели удаляю признак Interest_rate_sprea, который является утечкой данных."
      ],
      "metadata": {
        "id": "AIQnVfEN3631"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 4: Подготовка улучшенных данных\n",
        "# 1. Регрессия (Log + Clean Outliers)\n",
        "q99 = df_reg_clean['Price'].quantile(0.99)\n",
        "df_reg_imp = df_reg_clean[df_reg_clean['Price'] < q99].copy()\n",
        "df_reg_imp['log_Price'] = np.log1p(df_reg_imp['Price'])\n",
        "\n",
        "X_reg_imp = df_reg_imp.drop(columns=['Price', 'log_Price', 'Model'])\n",
        "y_reg_imp = df_reg_imp['log_Price']\n",
        "\n",
        "# Preprocessing\n",
        "X_reg_imp[num_cols_reg] = SimpleImputer(strategy='median').fit_transform(X_reg_imp[num_cols_reg])\n",
        "X_reg_imp[cat_cols_reg] = SimpleImputer(strategy='most_frequent').fit_transform(X_reg_imp[cat_cols_reg])\n",
        "\n",
        "for col in cat_cols_reg:\n",
        "    X_reg_imp[col] = le.fit_transform(X_reg_imp[col].astype(str))\n",
        "\n",
        "X_train_reg_imp, X_test_reg_imp, y_train_reg_imp, y_test_reg_imp = train_test_split(\n",
        "    X_reg_imp, y_reg_imp, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Классификация (Remove Leakage)\n",
        "X_cls_imp = df_class_sample.drop(columns=['Status', 'Interest_rate_spread'], errors='ignore')\n",
        "y_cls_imp = df_class_sample['Status']\n",
        "\n",
        "# Preprocessing\n",
        "cols_cls_imp_num = X_cls_imp.select_dtypes(include=['number']).columns\n",
        "cols_cls_imp_cat = X_cls_imp.select_dtypes(include=['object']).columns\n",
        "\n",
        "X_cls_imp[cols_cls_imp_num] = SimpleImputer(strategy='median').fit_transform(X_cls_imp[cols_cls_imp_num])\n",
        "X_cls_imp[cols_cls_imp_cat] = SimpleImputer(strategy='most_frequent').fit_transform(X_cls_imp[cols_cls_imp_cat])\n",
        "\n",
        "for col in cols_cls_imp_cat:\n",
        "    X_cls_imp[col] = le.fit_transform(X_cls_imp[col].astype(str))\n",
        "\n",
        "X_train_cls_imp, X_test_cls_imp, y_train_cls_imp, y_test_cls_imp = train_test_split(\n",
        "    X_cls_imp, y_cls_imp, test_size=0.25, random_state=42, stratify=y_cls_imp\n",
        ")\n",
        "\n",
        "print(\"Данные улучшены (Log-target, No Leakage).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu49KXd80khe",
        "outputId": "b4ab56c4-5ef7-456e-80aa-23b9591be817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные улучшены (Log-target, No Leakage).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Провожу подбор гиперпараметров для градиентного бустинга. Настраиваю ключевые параметры, отвечающие за баланс между качеством и скоростью обучения:\n",
        "\n",
        "*   n_estimators: количество деревьев. В отличие от леса, слишком большое число может привести к переобучению.\n",
        "*   learning_rate ($\\eta$): скорость обучения. Уменьшает вклад каждого дерева ($F_{new} = F_{old} + \\eta \\cdot \\text{Tree}$). Маленький $\\eta$ требует больше деревьев, но делает модель более робастной.\n",
        "*   max_depth: глубина каждого слабого дерева. В бустинге деревья обычно неглубокие (3-5), так как каждое из них должно исправлять лишь небольшую часть ошибок."
      ],
      "metadata": {
        "id": "KJRDbUoW4Fgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 5: GridSearch (Тюнинг Бустинга)\n",
        "params_gb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "# 1. Регрессия\n",
        "print(\"Start GridSearch Regression...\")\n",
        "grid_reg = GridSearchCV(GradientBoostingRegressor(random_state=42), params_gb, cv=3, scoring='r2', n_jobs=-1)\n",
        "grid_reg.fit(X_train_reg_imp, y_train_reg_imp)\n",
        "\n",
        "best_gb_reg = grid_reg.best_estimator_\n",
        "print(f\"Best Reg Params: {grid_reg.best_params_}\")\n",
        "\n",
        "# 2. Классификация\n",
        "print(\"\\nStart GridSearch Classification...\")\n",
        "grid_cls = GridSearchCV(GradientBoostingClassifier(random_state=42), params_gb, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid_cls.fit(X_train_cls_imp, y_train_cls_imp)\n",
        "\n",
        "best_gb_cls = grid_cls.best_estimator_\n",
        "print(f\"Best Cls Params: {grid_cls.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph0GcyEx0oQA",
        "outputId": "d26ea6fc-e907-467d-80c0-ec5aace89317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start GridSearch Regression...\n",
            "Best Reg Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "\n",
            "Start GridSearch Classification...\n",
            "Best Cls Params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 6: Оценка Улучшенного Бустинга\n",
        "# 1. Регрессия\n",
        "y_pred_log_imp = best_gb_reg.predict(X_test_reg_imp)\n",
        "y_pred_reg_imp = np.expm1(y_pred_log_imp)\n",
        "y_test_reg_orig = np.expm1(y_test_reg_imp)\n",
        "\n",
        "r2_imp = r2_score(y_test_reg_orig, y_pred_reg_imp)\n",
        "mae_imp = mean_absolute_error(y_test_reg_orig, y_pred_reg_imp)\n",
        "\n",
        "print(\"=== Улучшенный Gradient Boosting (Регрессия) ===\")\n",
        "print(f\"R2 Test:  {r2_imp:.4f} (Было: {r2_base:.4f})\")\n",
        "print(f\"MAE Test: {mae_imp:.2f}\")\n",
        "\n",
        "\n",
        "# 2. Классификация\n",
        "y_pred_cls_imp = best_gb_cls.predict(X_test_cls_imp)\n",
        "y_pred_proba_cls_imp = best_gb_cls.predict_proba(X_test_cls_imp)[:, 1]\n",
        "\n",
        "acc_imp = accuracy_score(y_test_cls_imp, y_pred_cls_imp)\n",
        "roc_imp = roc_auc_score(y_test_cls_imp, y_pred_proba_cls_imp)\n",
        "\n",
        "print(\"\\n=== Улучшенный Gradient Boosting (Классификация) ===\")\n",
        "print(f\"Accuracy: {acc_imp:.4f}\")\n",
        "print(f\"ROC-AUC:  {roc_imp:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7nRbe10pGY",
        "outputId": "41231259-ce27-4a5f-ee24-8e3cf155fdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Улучшенный Gradient Boosting (Регрессия) ===\n",
            "R2 Test:  0.9063 (Было: 0.6765)\n",
            "MAE Test: 2871.36\n",
            "\n",
            "=== Улучшенный Gradient Boosting (Классификация) ===\n",
            "Accuracy: 0.9965\n",
            "ROC-AUC:  0.9993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Анализ:**\n",
        "*   **Регрессия ($R^2 \\approx 0.906$):** Бустинг показал результат практически идентичный Случайному Лесу ($0.907$). Разница в тысячных долях - это статистическая погрешность. Оба ансамблевых метода показали себя как sota решения для этой задачи.\n",
        "*   **Классификация:** Стабильно высокий результат."
      ],
      "metadata": {
        "id": "k72SL23-1Q8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этой ячейке я реализую алгоритм Градиентного бустинга с нуля.\n",
        "\n",
        "1.  **Инициализация:** Я начинаю с простого предсказания $F_0(x)$, которое является константой для всех объектов.\n",
        "    *   Для регрессии это среднее значение $\\bar{y}$.\n",
        "    *   Для классификации это логарифм шансов (log-odds) среднего значения целевой переменной: $\\ln(\\frac{p}{1-p})$.\n",
        "\n",
        "2.  **Итеративное обучение:** В цикле я последовательно строю n_estimators деревьев. На каждой итерации $m$:\n",
        "    *   **Вычисляю псевдо-остатки (pseudo-residuals)**. Это антиградиент функции потерь по предсказаниям модели $F_{m-1}(x)$.\n",
        "        *   Для MSE (регрессия): $r_{im} = y_i - F_{m-1}(x_i)$.\n",
        "        *   Для LogLoss (классификация): $r_{im} = y_i - \\sigma(F_{m-1}(x_i))$.\n",
        "    *   **Обучаю слабое дерево** DecisionTreeRegressor предсказывать эти остатки. Важно, что даже для классификации здесь используется регрессор, так как мы предсказываем градиент (число).\n",
        "    *   **Обновляю модель:** Добавляю предсказания нового дерева к общей модели с учетом шага обучения $\\eta$:\n",
        "        $$F_m(x) = F_{m-1}(x) + \\eta \\cdot \\text{Tree}_m(x)$$\n",
        "\n",
        "3.  **Предсказание:** Финальное предсказание получается как сумма начального приближения и взвешенных предсказаний всех деревьев. Для классификации результат пропускается через сигмоиду $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, чтобы получить вероятности."
      ],
      "metadata": {
        "id": "vM-C2h4y4Ypf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ячейка 7: Класс MyGradientBoosting\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class MyGradientBoosting:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, task='regression'):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.task = task\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # 1. Инициализация (константой)\n",
        "        if self.task == 'regression':\n",
        "            self.initial_prediction = np.mean(y)\n",
        "            # Текущее предсказание для всех точек\n",
        "            current_preds = np.full(y.shape, self.initial_prediction)\n",
        "        else:\n",
        "            # Для классификации инициализирую log-odds\n",
        "            # Чтобы избежать log(0), добавляем epsilon\n",
        "            prob = np.clip(np.mean(y), 1e-10, 1-1e-10)\n",
        "            self.initial_prediction = np.log(prob / (1 - prob))\n",
        "            current_preds = np.full(y.shape, self.initial_prediction)\n",
        "\n",
        "        # 2. Основной цикл бустинга\n",
        "        for _ in range(self.n_estimators):\n",
        "\n",
        "            if self.task == 'regression':\n",
        "                # Антиградиент для MSE = (y - pred)\n",
        "                residuals = y - current_preds\n",
        "            else:\n",
        "                # Антиградиент для LogLoss = (y - sigmoid(pred))\n",
        "                # я работаю с сырыми logits (до сигмоиды)\n",
        "                proba = self._sigmoid(current_preds)\n",
        "                residuals = y - proba\n",
        "\n",
        "            # 3. Обучаем слабое дерево на остатках\n",
        "            # Всегда используем Regressor, так как предсказываем градиент (число)\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=42)\n",
        "            tree.fit(X, residuals)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            # 4. Обновляем предсказания\n",
        "            # pred_new = pred_old + lr * tree_pred\n",
        "            update = tree.predict(X)\n",
        "            current_preds += self.learning_rate * update\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "\n",
        "        # Начинаем с базового предсказания\n",
        "        preds = np.full(X.shape[0], self.initial_prediction)\n",
        "\n",
        "        # Суммируем предсказания всех деревьев с учетом learning rate\n",
        "        for tree in self.trees:\n",
        "            preds += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        # Для классификации возвращаем классы, для регрессии значения\n",
        "        if self.task == 'classification':\n",
        "            proba = self._sigmoid(preds)\n",
        "            return (proba > 0.5).astype(int)\n",
        "        else:\n",
        "            return preds\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Только для классификации\n",
        "        if self.task != 'classification':\n",
        "            raise ValueError(\"Predict_proba is only for classification\")\n",
        "        X = np.array(X)\n",
        "        preds = np.full(X.shape[0], self.initial_prediction)\n",
        "        for tree in self.trees:\n",
        "            preds += self.learning_rate * tree.predict(X)\n",
        "        return self._sigmoid(preds)\n",
        "\n",
        "print(\"Класс MyGradientBoosting создан.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QPr8shq1RM8",
        "outputId": "df03b571-2eed-41b1-cde9-f00c465a8bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Класс MyGradientBoosting создан.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Ячейка 8: Тестирование имплементации\n",
        "# === ЭТАП 4: Сравнение реализации  ===\n",
        "\n",
        "# Ограничим данные для скорости\n",
        "X_train_reg_imp_sub = X_train_reg_imp[:2000]\n",
        "y_train_reg_imp_sub = y_train_reg_imp[:2000]\n",
        "X_train_cls_imp_sub = X_train_cls_imp[:2000]\n",
        "y_train_cls_imp_sub = y_train_cls_imp[:2000]\n",
        "\n",
        "print(\"=== 4f-4i. Тест MyGradientBoosting на улучшеных данных ===\\n\")\n",
        "\n",
        "# -- 1. Регрессия (Improved) --\n",
        "# Параметры берем близкие к найденным в GridSearch\n",
        "my_gb_reg = MyGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3, task='regression')\n",
        "my_gb_reg.fit(X_train_reg_imp_sub, y_train_reg_imp_sub)\n",
        "\n",
        "# Sklearn для сравнения\n",
        "sk_gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "sk_gb_reg.fit(X_train_reg_imp_sub, y_train_reg_imp_sub)\n",
        "\n",
        "# Предсказание\n",
        "y_pred_log_my = my_gb_reg.predict(X_test_reg_imp)\n",
        "y_pred_reg_my = np.expm1(y_pred_log_my)\n",
        "\n",
        "y_pred_log_sk = sk_gb_reg.predict(X_test_reg_imp)\n",
        "y_pred_reg_sk = np.expm1(y_pred_log_sk)\n",
        "\n",
        "r2_my_imp = r2_score(y_test_reg_orig, y_pred_reg_my)\n",
        "r2_sk_imp = r2_score(y_test_reg_orig, y_pred_reg_sk)\n",
        "\n",
        "print(f\"[Регрессия Imp] MyGB R2:      {r2_my_imp:.4f}\")\n",
        "print(f\"                Sklearn R2:   {r2_sk_imp:.4f}\")\n",
        "print(\"                Вывод: Результаты должны быть очень близки.\")\n",
        "\n",
        "\n",
        "# -- 2. Классификация (Improved) --\n",
        "my_gb_cls = MyGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3, task='classification')\n",
        "my_gb_cls.fit(X_train_cls_imp_sub, y_train_cls_imp_sub)\n",
        "\n",
        "sk_gb_cls = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "sk_gb_cls.fit(X_train_cls_imp_sub, y_train_cls_imp_sub)\n",
        "\n",
        "y_pred_my = my_gb_cls.predict(X_test_cls_imp)\n",
        "y_pred_sk = sk_gb_cls.predict(X_test_cls_imp)\n",
        "\n",
        "print(f\"\\n[Классификация Imp] MyGB Acc:   {accuracy_score(y_test_cls_imp, y_pred_my):.4f}\")\n",
        "print(f\"                    Sklearn Acc: {accuracy_score(y_test_cls_imp, y_pred_sk):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BInYOJ_Y1T_A",
        "outputId": "ca02c132-b77e-4848-9c24-e992abda451d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 4f-4i. Тест MyGradientBoosting на улучшеных данных ===\n",
            "\n",
            "[Регрессия Imp] MyGB R2:      0.8810\n",
            "                Sklearn R2:   0.8810\n",
            "                Вывод: Результаты должны быть очень близки.\n",
            "\n",
            "[Классификация Imp] MyGB Acc:   0.9960\n",
            "                    Sklearn Acc: 0.9954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Глобальные выводы и сравнительный анализ (Лабораторные 1-5)\n",
        "\n",
        "В рамках цикла лабораторных работ было проведено исследование 5 основных семейств алгоритмов машинного обучения на двух типах задач:\n",
        "1.  **Регрессия:** Предсказание стоимости автомобиля (Car Sales).\n",
        "2.  **Классификация:** Прогнозирование дефолта по кредиту (Loan Default).\n",
        "\n",
        "Ниже представлен сравнительный анализ.\n",
        "\n",
        "## 1. Сводная таблица результатов\n",
        "\n",
        "Метрики приведены для настроенных моделей.\n",
        "\n",
        "| Алгоритм | Лаб. | Регрессия ($R^2$) | Классификация (ROC-AUC/Acc) | Особенности |\n",
        "| :--- | :---: | :---: | :---: | :--- |\n",
        "| **KNN** | №1 | 0.83 | ~0.92 | Сильный бейзлайн, но очень медленный на больших данных. Критичен к масштабированию. |\n",
        "| **Linear Models** | №2 | 0.69 | ~0.85 | Худший результат по качеству, но лучшая интерпретируемость. Критичны к выбросам и нелинейности. |\n",
        "| **Decision Tree** | №3 | 0.85 | ~0.99* | Склонно к переобучению. Идеально находит нелинейные паттерны и утечки данных. |\n",
        "| **Random Forest** | №4 | **0.91** | **~0.972** | Лучший результат из коробки. Стабилен, не требует масштабирования, гасит дисперсию. |\n",
        "| **Gradient Boosting** | №5 | **0.91** | **~0.971** | Высочайшая точность. Требует тонкой настройки learning rate и числа деревьев. |\n",
        "\n",
        "\n",
        "## 2. Анализ задачи Регрессии (Car Sales)\n",
        "Эта задача стала наиболее показательной для сравнения силы алгоритмов:\n",
        "*   **Провал линейных моделей:** Линейная регрессия показала результат $R^2=0.69$. ЭЗависимость цены от года/пробега не является прямой линией.\n",
        "*   **Эволюция деревьев:**\n",
        "    *   Одиночное дерево ($R^2=0.85$) уже справилось лучше линейной модели, но страдало от нестабильности.\n",
        "    *   Ансамбли (Лес и Бустинг) подняли планку до $R^2=0.91$. Бэггинг (Random Forest) устранил ошибки отдельных деревьев, а Бустинг (GBM) последовательно исправил сложные ошибки.\n",
        "*   **Роль препроцессинга:** Прирост качества (с 0.25 до 0.69 у линейной регрессии и с 0.71 до 0.91 у леса) дало логарифмирование целевой переменной log1p и удаление выбросов.\n",
        "\n",
        "## 3. Анализ задачи Классификации (Loan Default)\n",
        "*   **Проблема утечки данных:** Деревья решений сразу показали Accuracy $\\approx 100\\%$. Анализ важности признаков feature_importances_ выявил, что признак Interest_rate_spread почти однозначно определяет статус дефолта.\n",
        "*   **Устойчивость алгоритмов:**\n",
        "    *   **KNN** показал достойный результат ($0.92$), но был крайне медленным на выборке в 150к строк.\n",
        "    *   **Логистическая регрессия** ($0.85$) отстала от деревянных моделей, так как не смогла построить сложную разделяющую поверхность для идеально разделяющих признаков.\n",
        "    *   **Ансамбли** показали отличное разделение классов.\n",
        "\n",
        "## 4. Выводы по собственной имплементации\n",
        "Самостоятельная реализация алгоритмов на Python/NumPy позволила сделать следующие выводы:\n",
        "1.  **Сложность оптимизации:** Реализовать аналитическое решение (KNN, Дерево) проще, чем итеративное (Градиентный спуск в Linear/GBM). Настройка SGD требует тщательного подбора learning_rate, иначе модель не сходится.\n",
        "2.  **Скорость:** Python-реализации (особенно циклы в KNN и Деревьях) проигрывают оптимизированным C++/Cython реализациям sklearn в десятки раз. Векторизация операций numpy критически важна."
      ],
      "metadata": {
        "id": "-_fqnJp-1Wuu"
      }
    }
  ]
}